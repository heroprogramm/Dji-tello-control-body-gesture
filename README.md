# Dji-tello-control-body-gesture
ğŸš A multi-mode drone navigation project using computer vision and machine learning

This projectğŸ‘¨â€ğŸ’»  enables users to control a drone using multiple input methods powered by Computer Vision and Artificial Intelligence. The system can detect and respond to hand gestures, face movements, voice commands, and body positioning, making drone navigation more interactive and intuitive.

 Features

    ğŸ– Hand Gesture Control â€“ Control the drone with custom hand gestures trained on a deep learning model (hand_gesture_model_improve.h5).

    ğŸ™‚ Face Detection Control â€“ Use facial position and orientation to move the drone in specific directions.

    ğŸ™ Voice Command Control (Planned) â€“ Command the drone using simple voice instructions like take off, land, move forward, turn left, etc.

    ğŸš¶ Body Follow Mode â€“ The drone can autonomously follow the userâ€™s body movements and maintain a set distance.

    ğŸ›¡ Safety Feature â€“ Includes an emergency stop gesture or command to land the drone immediately.

ğŸ› ï¸ Technologies Used

    Programming Language: Python

    Libraries & Frameworks:

        OpenCV â€“ Image Processing & Object Detection

        Mediapipe / TensorFlow / Keras â€“ Gesture & Pose Recognition

        Pyttsx3 / SpeechRecognition â€“ (For planned voice commands)

        DJI Tello SDK or DroneKit â€“ Drone communication

        NumPy, Pandas â€“ Data handling and processing

ğŸ“¦ Drone-Control
 â”£ ğŸ“‚ custom Hand Gesture model
 â”ƒ â”— hand_gesture_model_improve.h5
 â”£ ğŸ“‚ datasets
 â”£ ğŸ“œ main_hand_gesture.py
 â”£ ğŸ“œ main_face_detection.py
 â”£ ğŸ“œ main_body_follow.py
 â”£ ğŸ“œ main_voice_command.py (planned)
 â”£ ğŸ“œ requirements.txt
 â”— ğŸ“œ README.md

ğŸ”® Future Enhancements

    Full integration of voice command control

    Improved gesture recognition accuracy

    Mobile application support for on-the-go control

    Advanced obstacle avoidance using depth sensors
